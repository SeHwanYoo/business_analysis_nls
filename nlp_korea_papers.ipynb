{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 논문 Fasttext 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% PDF reader \n",
    "\n",
    "from io import StringIO\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import os\n",
    " \n",
    "path = \"D:/work/paper_ko/datasets/\"\n",
    "texts = '' \n",
    "for i in os.listdir(path):\n",
    "    # print(i)\n",
    "    \n",
    "    if '.pdf' in i:\n",
    "        text = '\\n' + read_pdf_PDFMINER(path + i)\n",
    "    \n",
    "# %%\n",
    "\n",
    "def read_pdf_PDFMINER(pdf_file_path):\n",
    "    \"\"\"\n",
    "    pdf_file_path: 'dir/aaa.pdf'로 구성된 path로부터 \n",
    "    내부의 text 파일을 모두 읽어서 스트링을 리턴함.\n",
    "    https://pdfminersix.readthedocs.io/en/latest/tutorials/composable.html\n",
    "    \"\"\"\n",
    "    output_string = StringIO()\n",
    "    with open(pdf_file_path, 'rb') as f:\n",
    "        parser = PDFParser(f)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "    return str(output_string.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% stopwords \n",
    "\n",
    "f = open(path + 'stopwords.txt', mode='r', encoding='utf-8')\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "stop_words = ''\n",
    "for ll in lines:\n",
    "    # print(ll)\n",
    "    stop_words += ' ' + ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 특수문자 처리\n",
    "import re\n",
    "\n",
    "parse_text = text\n",
    "\n",
    "clean_lines = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', parse_text)\n",
    "p = re.compile('[^0-9]')\n",
    "clean_lines = \"\".join(p.findall(clean_lines))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상한 데이터가 많아서 \n",
    "# 한번 더 처리\n",
    "\n",
    "lines = [] \n",
    "for cc in clean_lines.split('\\n\\n'):\n",
    "    print(cc)\n",
    "    if cc != '\\n' and cc != '\\n\\n' and cc != ' ' and cc != '  ' and cc != '   ':\n",
    "        lines.append(cc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt() \n",
    "results = []\n",
    "for ll in lines:\n",
    "    results.append(okt.nouns(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "# tt = twitter.morphs(\"테스트를 해본다\")\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for cc in clean_lists:\n",
    "    t_cc = twitter.morphs(cc)\n",
    "    \n",
    "    for tt in token_text:\n",
    "        \n",
    "        if tt not in stop_words:\n",
    "            results.append(tt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "ft_model = FastText(results, size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ko_model.wv.most_similar('멘토', topn=10))\n",
    "print(ft_model.wv.similarity('멘토링', '창업'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 를 통한 시각화 \n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "words = [\n",
    "    '멘토링',\n",
    "    '멘토',\n",
    "    '멘티',\n",
    "    '창업',\n",
    "    '취업',\n",
    "    '기업가정신'\n",
    "]\n",
    "# matplotplib 폰트 설정을 안해서 그래프에서 한국어 라벨이 깨져서 아래 단어로 임시처리했습니다. ㅠ\n",
    "word_labels = [\n",
    "    'mentoring',\n",
    "    'mento',\n",
    "    'menti',\n",
    "    'startup',\n",
    "    'job', \n",
    "    'entrepreneurship'\n",
    "]\n",
    "pca = PCA(n_components=2)\n",
    "xys = pca.fit_transform([ft_model.wv.word_vec(w) for w in words])\n",
    "xs = xys[:,0]\n",
    "ys = xys[:,1]\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(xs, ys, marker='o')\n",
    "for i, v in enumerate(word_labels):\n",
    "    plt.annotate(v, xy=(xs[i], ys[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
