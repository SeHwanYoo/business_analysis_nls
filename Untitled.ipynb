{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "PATH = 'd:/Github/nls/paper/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 504: expected 31 fields, saw 32\\n'\n",
      "b'Skipping line 507: expected 31 fields, saw 32\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(940, 31)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "data = []\n",
    "for i in range(10):\n",
    "    # print(PATH + 'dataset/scopus(' + str(i) + ').csv')\n",
    "    df = pd.read_csv(PATH + 'dataset/scopus(' + str(i) + ').csv', error_bad_lines=False)\n",
    "\n",
    "    # print(df.dtype)\n",
    "    for idx, val in df.iterrows():\n",
    "        # print(val)\n",
    "\n",
    "        for vv in val['Abstract'].split('.'):\n",
    "            data.append(vv)\n",
    "    \n",
    "\n",
    "print(df.shape) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       173\n",
      "1       235\n",
      "2       124\n",
      "3       199\n",
      "4       328\n",
      "       ... \n",
      "935     306\n",
      "936     355\n",
      "937     276\n",
      "938     388\n",
      "939    1181\n",
      "Name: word_len, Length: 940, dtype: int64\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# print(df.shape)\n",
    "%matplotlib qt5 \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df['word_len'] = df['Abstract'].apply(lambda words : len(words.split(' ')))\n",
    "print(df['word_len'])\n",
    "\n",
    "max_seq_len = np.round(df['word_len'].mean() + df['word_len'].std()).astype(int)\n",
    "print(max_seq_len)\n",
    "\n",
    "sns.distplot(df['word_len'], hist=True, kde=True, color='b', label='words length')\n",
    "plt.title('Distribution of Words Count in Abstracts')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# max_seq_len = np.round(df[\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125269\n",
      "This research paper is to evaluate and present the results from the online survey conducted in various Indian manufacturing small and medium enterprises (SMEs), mainly to focus critical success factors (CSFs) for implementing Six Sigma in Indian manufacturing small and medium enterprises and identified most crucial critical success factors for Six Sigma implementation in SMEs of manufacturing sector in India\n",
      "--------\n",
      " The paper is based on survey questionnaire apposite for Indian manufacturing SMEs and the result investigation of the present study is based on factor analysis and descriptive statistics\n",
      "--------\n",
      " The results are investigated by the factor analysis and reveal the impact of different CSFs on the Indian manufacturing SMEs\n"
     ]
    }
   ],
   "source": [
    "# print(df.head())\n",
    "\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "print('--------')\n",
    "print(data[1])\n",
    "print('--------')\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(data[:10])\n",
    "import re\n",
    "words = []\n",
    "for idx, d in enumerate(data):\n",
    "    # print(type(d))\n",
    "    clean = re.sub(r'[=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》©]', '', d)\n",
    "    lower = clean.lower()\n",
    "    # print(str(idx) + '         ' + lower)\n",
    "    \n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    word = shortword.sub('', lower)\n",
    "    \n",
    "    words.append(word.strip())\n",
    "\n",
    "# print(words[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# stopwords: remove trash words \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "pst = PorterStemmer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['research', 'paper', 'evalu', 'present', 'result', 'onlin', 'survey', 'conduct', 'variou', 'indian', 'manufactur', 'small', 'medium', 'enterpris', 'sme', 'mainli', 'focu', 'critic', 'success', 'factor', 'csf', 'implement', 'sigma', 'indian', 'manufactur', 'small', 'medium', 'enterpris', 'identifi', 'crucial', 'critic', 'success', 'factor', 'sigma', 'implement', 'sme', 'manufactur', 'sector', 'india'], ['paper', 'base', 'survey', 'questionnair', 'apposit', 'indian', 'manufactur', 'sme', 'result', 'investig', 'present', 'studi', 'base', 'factor', 'analysi', 'descript', 'statist'], ['result', 'investig', 'factor', 'analysi', 'reveal', 'impact', 'differ', 'csf', 'indian', 'manufactur', 'sme'], ['tjprc'], []]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = [word_tokenize(w) for w in words]\n",
    "\n",
    "tokens_stopwords = [[w for w in t if not w in stop_words] for t in tokens]\n",
    "\n",
    "tokens_stemmed = [[pst.stem(w) for w in token if len(w) > 3] for token in tokens_stopwords]\n",
    "\n",
    "# tokens_stemmed = [[pst.stem(w) for w in token] for token in tokens_stopwords]\n",
    "\n",
    "print(tokens_stemmed[:5])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 unique words\n",
      "{'conduct': 0, 'critic': 1, 'crucial': 2, 'csf': 3, 'enterpris': 4, 'evalu': 5, 'factor': 6, 'focu': 7, 'identifi': 8, 'implement': 9, 'india': 10, 'indian': 11, 'mainli': 12, 'manufactur': 13, 'medium': 14, 'onlin': 15, 'paper': 16, 'present': 17, 'research': 18, 'result': 19, 'sector': 20, 'sigma': 21, 'small': 22, 'sme': 23, 'success': 24, 'survey': 25, 'variou': 26, 'UNK': 27}\n",
      "['conduct' 'critic' 'crucial' 'csf' 'enterpris' 'evalu' 'factor' 'focu'\n",
      " 'identifi' 'implement' 'india' 'indian' 'mainli' 'manufactur' 'medium'\n",
      " 'onlin' 'paper' 'present' 'research' 'result' 'sector' 'sigma' 'small'\n",
      " 'sme' 'success' 'survey' 'variou' 'UNK']\n",
      "------------------------------\n",
      "['research', 'paper', 'evalu', 'present', 'result', 'onlin', 'survey', 'conduct', 'variou', 'indian', 'manufactur', 'small', 'medium', 'enterpris', 'sme', 'mainli', 'focu', 'critic', 'success', 'factor']\n",
      "[18 16  5 17 19 15 25  0 26 11 13 22 14  4 23 12  7  1 24  6]\n"
     ]
    }
   ],
   "source": [
    "# print(tokens_stemmed[0])\n",
    "# print(len(tokens_stemmed))\n",
    "\n",
    "import numpy as np\n",
    "# print(np.shape(tokens_stemmed))\n",
    "\n",
    "train_text_X = tokens_stemmed[0]\n",
    "\n",
    "# print(train_text_X)\n",
    "\n",
    "# Creating NLS\n",
    "\n",
    "vocab = sorted(list(set(train_text_X)))\n",
    "vocab.append('UNK')\n",
    "\n",
    "print('{} unique words'.format(len(vocab)))\n",
    "\n",
    "word2idx = {u : i for i, u in enumerate(vocab)}\n",
    "print(word2idx)\n",
    "\n",
    "idx2word = np.array(vocab)\n",
    "print(idx2word)\n",
    "\n",
    "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
    "\n",
    "print('------------------------------')\n",
    "print(train_text_X[:20])\n",
    "print(text_as_int[:20])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['research' 'paper' 'evalu' 'present' 'result' 'onlin' 'survey' 'conduct'\n",
      " 'variou' 'indian' 'manufactur' 'small' 'medium' 'enterpris' 'sme'\n",
      " 'mainli' 'focu' 'critic' 'success' 'factor' 'csf' 'implement' 'sigma'\n",
      " 'indian' 'manufactur' 'small']\n",
      "[18 16  5 17 19 15 25  0 26 11 13 22 14  4 23 12  7  1 24  6  3  9 21 11\n",
      " 13 22]\n",
      "['research' 'paper' 'evalu' 'present' 'result' 'onlin' 'survey' 'conduct'\n",
      " 'variou' 'indian' 'manufactur' 'small' 'medium' 'enterpris' 'sme'\n",
      " 'mainli' 'focu' 'critic' 'success' 'factor' 'csf' 'implement' 'sigma'\n",
      " 'indian' 'manufactur']\n",
      "[18 16  5 17 19 15 25  0 26 11 13 22 14  4 23 12  7  1 24  6  3  9 21 11\n",
      " 13]\n",
      "small\n",
      "22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 25, 100)           2800      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 25, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                2828      \n",
      "=================================================================\n",
      "Total params: 166,428\n",
      "Trainable params: 166,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DATASET\n",
    "seq_length = 25\n",
    "examples_per_epoch = len(text_as_int)\n",
    "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "# print(examples_per_epoch)\n",
    "# print(sentence_dataset)\n",
    "sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# print(sentence_dataset)\n",
    "for item in sentence_dataset.take(1):\n",
    "    print(idx2word[item.numpy()])\n",
    "    print(item.numpy())\n",
    "\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    return [chunk[:-1], chunk[-1]]\n",
    "\n",
    "train_dataset = sentence_dataset.map(split_input_target)\n",
    "for x, y in train_dataset.take(1):\n",
    "    print(idx2word[x.numpy()])\n",
    "    print(x.numpy())\n",
    "    print(idx2word[y.numpy()])\n",
    "    print(y.numpy())\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = examples_per_epoch\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "total_words = len(vocab)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(units=100),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1950 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'logs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-ace9fc9e0bf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mtestmodelcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLambdaCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtestmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtestmodelcb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;31m# history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, verbose=2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1102\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m         \u001b[1;31m# Run validation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'logs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def testmodel(epoch, logs):\n",
    "    \n",
    "    if epoch % 5 != 0 and epoch != 49:\n",
    "        return\n",
    "    test_sentence = train_text[0]\n",
    "\n",
    "    next_words = 100\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        test_text_X = test_sentence.split(' ')[-seq_length:]\n",
    "        test_text_X = np.array([word2idx[c] if c in word2idx else word2idx ['UNK'] for c in test_text_X])\n",
    "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
    "        output_idx = model.predict_classes(test_text_X)\n",
    "        test_sentence += ' ' + idx2word[output_idx[0]]\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(test_sentence)\n",
    "    print()\n",
    "\n",
    "# print(steps_per_epoch)\n",
    "\n",
    "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
    "history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)\n",
    "# history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions = PATH + 'dataset/questions.txt'\n",
    "\n",
    "def model_accuracy(model):\n",
    "\n",
    "    accuracy = model.accuracy(questions)\n",
    "    \n",
    "    sum_corr = len(accuracy[-1]['correct'])\n",
    "    sum_incorr = len(accuracy[-1]['incorrect'])\n",
    "    total = sum_corr + sum_incorr\n",
    "    percent = lambda a: a / total * 100\n",
    "\n",
    "    return percent(sum_corr) \n",
    "\n",
    "def w2v_model_accuracy(model):\n",
    "\n",
    "    accuracy = model.accuracy(questions)\n",
    "    \n",
    "    sum_corr = len(accuracy[-1]['correct'])\n",
    "    sum_incorr = len(accuracy[-1]['incorrect'])\n",
    "    total = sum_corr + sum_incorr\n",
    "    percent = lambda a: a / total * 100\n",
    "    \n",
    "    print('Total sentences: {}, Correct: {:.2f}%, Incorrect: {:.2f}%'.format(total, percent(sum_corr), percent(sum_incorr)))\n",
    "\n",
    "\n",
    "# w2v_model_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-409c9e96ef31>:16: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.wv.accuracy() instead).\n",
      "  accuracy = model.accuracy(questions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 806, Correct: 63.65%, Incorrect: 36.35%\n"
     ]
    }
   ],
   "source": [
    "# skip-gram method\n",
    "from gensim.models import FastText\n",
    "\n",
    "# 65.38 \n",
    "# model = FastText(tokens_stemmed, size=130, min_count=10, window=15, workers=4, sg=1)\n",
    "\n",
    "model = FastText(tokens_stemmed, size=130, min_count=10, window=19, workers=4, sg=1)\n",
    "\n",
    "w2v_model_accuracy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-409c9e96ef31>:16: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.wv.accuracy() instead).\n",
      "  accuracy = model.accuracy(questions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 806, Correct: 35.61%, Incorrect: 64.39%\n"
     ]
    }
   ],
   "source": [
    "# CBOW method \n",
    "\n",
    "model = FastText(tokens_stemmed, size=130, min_count=10, window=15, workers=4, sg=0)\n",
    "\n",
    "w2v_model_accuracy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "max_acc = 0\n",
    "max_size = 0\n",
    "max_window = 0\n",
    "max_min_count = 0 \n",
    "\n",
    "down_cnt = 0\n",
    "\n",
    "\n",
    "for ii in range(70, 300, 2):\n",
    "    model = FastText(tokens_stemmed, size=ii, workers=4, sg=1)\n",
    "\n",
    "    acc = model_accuracy(model)\n",
    "    \n",
    "    print('Correct: {:.2f}%'.format(acc))\n",
    "\n",
    "    if acc >= 60:\n",
    "        print('HOW MUCH? ', kk)\n",
    "\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "        max_size = ii\n",
    "        # max_window += 1\n",
    "        # max_min_count += 1\n",
    "        \n",
    "    elif max_acc > acc:\n",
    "\n",
    "        if down_cnt <= 15:\n",
    "            down_cnt += 1\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "print('Best Size is : ', max_size)\n",
    "\n",
    "for kk in range(5, 30):\n",
    "    model = FastText(tokens_stemmed, size=max_size, window=kk, workers=4, sg=1)\n",
    "\n",
    "    acc = model_accuracy(model)\n",
    "    print('Correct: {:.2f}%'.format(acc))\n",
    "\n",
    "    if acc >= 60:\n",
    "        print('HOW MUCH? ', kk)\n",
    "\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "        # max_size = ii\n",
    "        max_window += kk \n",
    "        # max_min_count += 1\n",
    "        \n",
    "    elif max_acc > acc:\n",
    "        if down_cnt <= 15:\n",
    "            down_cnt += 1\n",
    "        else:\n",
    "            break \n",
    "\n",
    "print('Best Window is :', max_window) \n",
    "\n",
    "for ww in range(5, 10):\n",
    "    model = FastText(tokens_stemmed, size=max_size, window=max_window, workers=ww, sg=1)\n",
    "\n",
    "    acc = model_accuracy(model)\n",
    "    print('Correct: {:.2f}%'.format(acc))\n",
    "\n",
    "    if acc >= 60:\n",
    "        print('HOW MUCH? ', kk)\n",
    "\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "        # max_size = ii\n",
    "        # max_window += kk \n",
    "        max_min_count += ww \n",
    "        \n",
    "    elif max_acc > acc:\n",
    "        if down_cnt <= 15:\n",
    "            down_cnt += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print('Best Max Min Count is ', max_min_count)\n",
    "            \n",
    "    \n",
    "    \n",
    "    # if acc >= 80:\n",
    "    #     print(ii)\n",
    "    #     break \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "D:/Anaconda\\python.exe",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
